---
title: 'Understanding Uncertainty'
subtitle: 'How can we measure or quantify uncertainty?'
format: html
execute: 
  eval: true
  echo: false
---

```{r}
#| label: setup
#| include: false

load('data/lab_objs.Rdata')

```

## Introduction

Often when faced with a decision the best choice is unclear.
Nowadays it is *de rigueur* to claim that all decisions are evidence-based, or data-driven.
But even when this is as true as it can possibly be, it is often still uncertain what the best course of action will be.
Using a simplistic scenario based approach this activity is intended to bring to the surface the types of issues that arise in an uncertain system.
Using an interactive simulation we can get an idea of how data can vary, and how we can learn to be comfortable with uncertainty by reframing problems and recognizing that our data allow us to moderate our expectations.

The activity is split into two parts.
The first part focuses on developing an understanding of how data quality affects our understanding of our system.

In the second part things become a bit more theoretical.
It is where those of us who like to know *"why"* will begin to engage with the answers to those questions.
We will explore how probability distributions to help understand why the more practical ideas from the first session work.

By the end of the activity we should start to see the importance of engaging with uncertainty.
Through our discussions we should begin to see possibilities for going beyond just engaging, to actively embracing uncertainty.

At several points during the activity you will be asked to record the thoughts, ideas, and questions surfaced by your group, as well as some of the values output by the app.
Please write anything that occurs down because some of these will be referred at later points in the activity.
You can choose whether to keep your own notes or to nominate a scribe for the group.

## Scenario

Consider a business that sells a wide range of items related to the field of *Jiggery-pokery*.
Their stock include specialist items like *flooglebinders*, *dooflickers*, and *hingmies*, well as more prosaic items like *widgets*, *watsits*, and *doodahs*.
The great thing about this stock is that all items sell at (their own) consistent rates throughout the year.
That is to say, they are entirely unaffected by seasonal or cyclic trends, or world events, such as economic catastrophes or pandemics.
The average weekly value of sales made by this business is exactly £100.
This figure is the *average* over the entire history of the business, and is known to be exactly correct.

The Boss (whose name shall also be Boss) of this business wants to increase the average sales, so decides to implement a new marketing campaign designed to more effectively target a different subset of its customers - hopefully ones that spend more money.

In the first week following the new marketing campaign they find their average sale rose to £109.66.
Higher indeed than the expected average of £100.

![Profit for the week following website changes](../images/d2-s1_improved-sales.png){#fig-improved-profit fig-align="left" width="40%" alt="The profit observed in the week following improvements to the website was £109.66"}

Boss congratulates the team, and declares their new marketing strategy a success.

Was Boss right to do so?

::: {.callout-tip icon="false"}
## **`r i()` Formulate business relevant questions**

With your colleague(s) discuss what other information you would like to know before agreeing or disagreeing with The Boss.

Create a list of as many important questions you can think of, that you would like answered.
:::

## Part 1 --- How can we be sure?

We are going to explore how different possible answers to those questions might affect how we come to a decision.

::: callout-caution
### Assumptions for our scenario

The scenario is deliberately simple.

There is a historic average, which should consider as a baseline for our business, and should be taken at face value.

This baseline is based on the business' performance prior to the "marketing campaign".
The observed value should also be taken at face value as the performance after the marketing campaign.
The purpose of it is to provide a scaffold for discussions, not to get tied up with the real life business details about how flooglebinder sales might respond to external, or seasonal, or etc., factors.
:::

### Average expectation

![Sample average (observed) vs expected average](../images/d2-s1_improved-sales.png){#fig-exp-vs-sample alt="Plot showing a horizontal line labelled \"Expected\" at y = £100, and an \"Observed\" value of £109.66 " fig-align="left" width="40%"}

@fig-exp-vs-sample above shows a horizontal gold line at £100.
This indicates the all-time historic average for our company.
For the purposes of this exercise, we shall consider this an unambiguous "truth" of what our business has made in an average week.
It is therefore not a **statistic**, since statistics are values that are used to **estimate** "true" parameters.

Looking at this we see that the recent sample yielded an average value higher than the historic average.

::: {.callout-tip icon="false"}
### **`r i('Questions')`**

1.  How much would you trust that this increased value is likely to continue in future weeks?
2.  What factors can you identify that might support your answer for question 1?
    i.  If you would be enthusiastic that this is a strong result to take to The Boss, create a list of reasons that justify your belief.
    ii. If you are less than enthusiastic create a list of reasons, along with ideas that would increase your willingness to go to The Boss.
    iii. If you are not sure, create both of the above lists!
:::

Now that you have created a list follow this [link](https://cc-stats.shinyapps.io/uncertainty).
The plot on the left is the same as that in @fig-exp-vs-sample, but with a different "observed" value.
The value you see will be unique to your team, and is the result that you will be considering.
If it is above expectation, The Boss will be happy, if not, they won't!
For our purposes, we will assume a change of at least **1 percent** to be an outcome that has real importance for our business.
It is your job to decide whether the marketing campaign was a success or a failure, and choose to either agree or disagree with The Boss.

::: {.callout-tip icon="false"}
#### **`r i('Discuss')`**

What are your impressions of this new value with regards to the marketing campaign, and The Boss' assumed reaction?
:::

Now create a table with a column called `n = 5` (we will add a few more columns as we go).
You can create your table on paper or using the desktop app of your choice.
Record your value in your table.
Please keep your notes close at hand for future reference.

Now... unbeknownst to Boss we have somehow acquired superhero like powers that allow us to see how/whether sales continue to respond to our marketing campaign.
We have the ability to simulate what the observed value might be like in future weeks, by pressing the ![](../images/btn_new-sample){width="108" alt="\"New sample\""} button.
Press it several times, and, after each press, record the new observed value.
You should see that the observed value changes - possibly by quite a lot!
Notice also that the two graphs on the right respond to any changes---more about these later.

There are several controls in the left sidebar that we can experiment with to help understand some of the factors that influence the value, and possibly indicate how much faith we should have in our results.
There are quite a few controls, so lets explore them one at a time.

### Sample size {#sample-size}

![](../images/input_n-sales-5.png){#fig-n-sales-input display="inline" width="300" alt="sample size input, set to n = 5"}

The first control we will look at on the left is labelled "Number of sales".
It is currently set at `5`, and indicates that five sales were made during the week.
You have just seen how the observed value changes, sometimes quite dramatically, week to week.

::: {.callout-tip icon="false"}
#### **`r i('Discuss')`**

Given that our information is based on five customers discuss why the fact that this is based on 5 sales causes the value to jump around so much?

-   How likely is it that they represent our "average customer"?
-   How confident would you feel making future decisions based on these five sales?
-   What message might you have for The Boss?
:::

Let's have a look at the individual sales, tick the "Show sales" box.

![](../images/input_cb-show-sales.png){width="300" alt="two checkboxes labeled \"Margin of error\" and \"Sales\". The bottom box, \"Sales\" has been checked"}

The small red points that appeared on the plot show the values of each of the sales made.
It might now be clearer that the large red point labelled "Observed value" actually represents the mean of all the sales.
Press the ![](../images/btn_new-sample.png){width="108" alt="\"New sample\""} button several more times.

::: {.callout-tip icon="false"}
#### **`r i('Discuss')`**

How do you think increasing the number of sales will affect the observed value?

How do you think increasing the number of sales will affect your opinion of the marketing strategy?

Remember, The Boss has no idea about any of these future outcomes.

How might you convey these insights to The Boss?
:::

In your table, create a new column with the header `n = 20` .
Now change the "Number of sales" input to 20, and try several new samples (press ![](../images/btn_new-sample.png){width="107" alt="\"New sample\""}).
Record each of the new values in your table.

::: {.callout-tip icon="false"}
#### **`r i('Discuss')`**

Compare the values from each of the two columns of your table.

Discuss how the values are similar and how they are different.

What does having more sales mean in terms of approaching The Boss with a decision?

What if we had 100 sales?
Add one more column `n = 100` and compare those values.
:::

At this point The Boss tracks you down, and wants confirmation that they were correct.
What can you tell them?

-   Has the marketing campaign affected the customer base?
-   In what direction---are they spending more or less than before?
-   If the previous average was £100, what is your best guess at the new average?
-   Write down what you think the new average sale is.

### Precision

The Boss was somewhat impressed by your investigative skills, and is almost content to take your word for it.
However, they realize that the average sale will vary week to week.
They want you to give a best guess that somehow accounts for this variation.

Hopefully we were able to see, and articulate in our teams, how more sales affected our average sales value.
However, just having more sales might not be enough to allow us to come to a decision.
If you carried out the last instruction in the box above, you should have number of sales set to 100.
Change the input below that called "Spread of values", to 50, hit ![](../images/btn_new-sample.png){width="107" alt="\"New sample\""} a few more times.

Hmm, things just got unpredictable again, right?

::: {.callout-tip icon="false"}
#### **`r i('Discuss')`**

What might this "Spread of values" input represent in for our business---How about in a real-world/business context?
Can you think of a parallel in your own line of work?

::: {.callout collapse="true"}
##### Hint

Consider the price range of the stock.
How similar are multiple sales likely to be?
:::
:::

The width of the range of values has an important effect on our average sale value.
Suppose everything we sold cost £100.
With 5 sales, we could be 100% certain of what our average sales would be £100.

::: {.callout-tip icon="false"}
#### **`r i('Discuss')`**

Consider the following business:

1.  Budget store -- prices range between £80 - £90
2.  Generalist store -- prices range between £85 - £105
3.  Specialist high-end store -- prices range between £105 - £110

Suppose each business makes the same number of sales.

How would this range of stock prices affect the average sales values?

Which business should be able to make the most precise projections for the future?

Which would be least accurate?
:::

Ok, time for a slight confession---I'm afraid the task above was a little bit of a mean trick!
To be fair, though, there was a clue in the section header.
See, given a large enough number of sales, all three businesses could create similarly **accurate** projections.
This is what the mean does.
While the estimate may jump about wildly when the sample size (number of sales) is small, it tends to do so around the actual true mean.
Where the businesses would differ is in the **precision** of their projections.
In order to increase our precision we have to reduce the range within which the estimate jumps around.

The current input settings should be as show on the top left To get a better understanding of how the "spread" relates to the precision, tick the "Show margin of error" box (bottom left).
This will make a few changes.
First you will notice there are now error bars extending form the red point on the plot.
One other thing that has changed is the box under the plot labelled "Confidence interval" now show the range of the margin of error (MoE).

::: {layout="[[30, -5, 30], [30, -5, 30]]" layout-valign="center"}
![](../images/input-n100-s50.png){width="250" alt="Two numeric inputs for changing the number of sales (top - displaying 100) and the \"spread\" of the data (bottom - displaying 50)."}

![](../images/vb_ci-unknown.png){width="250" alt="Value box, showing a confidence interval of unknown value. The colour of the title bar is yellow, indicating the value is unknown, or does not cover the expected value of 100."}

![](../images/input_cb-show-moe-sales.png){width="250" alt="Two checkboxes labeled \"Margin of error\" and \"Sales\". Both boxes have been checked."}

![](../images/vb_ci-known-sig.png){width="250" alt="Value box displaying a margin of error of 102.5-118.2. The title bar of the box is green indicating that the margin of error does not cover the expected value of 100."}
:::

::: {.callout-tip icon="false"}
#### **`r i('Task')`**

Experiment with the sample size and spread values to manipulate the margin of error.

Which combinations create a wide MoE, and which create a narrow MoE.

Record a few observations---they might come in handy later.

Also have a think about what those combinations might mean in a real-world context.
For example, consider the three business types above.

::: {.callout icon="false" collapse="true"}
##### **Hint**

If the spread is small, how many sales do we need to have a small MoE?
:::
:::

We can now see that while the sample size can help us guess what the average is, how much future weeks deviate from our guess depends on the spread, or the **variation** in our sales values.

The next time The Boss comes calling we can now give an average and a likely upper and lower limit for our estimate.
"That's great," says Boss, "But what do you mean by *likely* limits?"

### Margin of error {#margin-of-error}

What we have been tip-toeing around is the concept of a **confidence interval** (CI).
The language for dealing with these is a bit statsy and confusing, so we'll try and set some expectations first which we will then try and test to confirm or refute.

::: {.callout-tip icon="false"}
#### **`r i('Discuss')`**

You inform Boss that the margin of error you quoted is actually a 95% confidence interval.
"Very good," says Boss, "Why only 95%? Why not 99%? Actually, do we even need to be 95% confident? Can't we settle for 90%?"

Well, these are all good questions.

Before you change the CI, discuss what will happen to the error bars when you change to 90% or 99%.
Make a note of your expectations.

What might the pros and cons be for each option?
What advantages might a 90% CI have over a 99% CI and vice versa?
Make a note of any ideas you have for these also.
:::

So far we have been dealing with a 95%.
Go ahead ahead now and change the selection.

![](../images/input_rb-ci-95.png){width="250" alt="Radio button allowing choices for the margin of error between 90%, 95% and 99%. Displays 95% as selected"}

Watch what happens to the error bars as you do this.
Compare the changes to the notes you took in your previous discussion.
Was your expectation correct?
If not try and explain why.

Now here comes the hard stuff.
It is a very common misunderstanding that a 95% confidence interval means that there is a 95% chance that the true value is within the CI.
So lets break it down.

1.  What do we mean by "true value"?
    -   In our case, the true value is the **actual** average sale for our customer base
    -   (More generally this would be the underlying population parameter.)
2.  We don't know what the true value is, that is why we are need statistics.
    -   But every week, our CI is slightly different, so that proves the misunderstanding is false.
    -   For several different CIs to all have a 95% chance of containing the true value doesn't make sense
3.  If we turn that idea upside down and instead say...
    -   Of the CIs we compute for the next $X$ weeks, 95% of them will cover the true mean.

Hmmm, what does that actually mean?
Well for one thing, it means even if we have everything in place exactly as we would like, we might still be wrong.

The more important question is how can we use this concept?
Remember our question is to investigate whether our marketing campaign has caused a shift in our customer base.
If we can show, within some level of confidence, that our new customer base spends a different amount on average than our old customer base, then we can conclude that our marketing campaign did cause a shift.

So, based on our new sample we have an estimate of our customer's spends.
And we have some margin of error around that mean.
Now, here's the clever bit.

::: columns
::: column
::: {#fig-ci-coverage layout-ncol="1"}
![CI covers expected value](../images/d2-s1_plot_ci-covers-expected.png){alt="Observed value is 90, but the upper limit of the margin of error extends to 105."}

![CI does not cover expected value](../images/d2-s1_plot_ci-misses-expected.png){alt="Observed value is 93, and the upper limit of the margin of error stops at 98, below the expected value."}

Have the week's sales actually been different from the expectation?
:::
:::

::: column
-   If that CI covers the expected average of the *old* customer base,
    -   That means that the old average could be the true mean for the new base.
    -   And, if that is true, we are forced to admit that there is no evidence that the customer base has been influenced by any meaningful amount.

<br>

-   Conversely, if the CI **does not** cover the old average...
    -   It suggests that the new average is sufficiently far from the old average

    -   And that the spread of the data is accounted for well enough that we have evidence that the new customers are indeed from a different group than the old customers.
:::
:::

::: {#box-best-ci-disc .callout-tip icon="false"}
#### **`r i('Discussion')`**

Spend five minutes working through the details of this stuff.
Given that using a 90% CI makes it easier for the marketing department to claim success, why would anyone use any other CI threshold?

::: {.callout collapse="true"}
##### **Hint**

Why not use an 80% CI?
Or a 50% CI?
:::
:::

### Summary

So far we have built our understanding that the average weekly sales value is centred around some value that is what our customer base spends on average per week.
Sometimes customers spend more than this central value, and sometimes they spend less.
However, the final point to realise, is that no matter what each individual spends, the total weekly sales value will tend to cluster around the expected average for our business (@fig-summary-bell-curve).

![Possible distibution of average weekly sales.](../images/d2-s1_plot_part-1-summary.png){#fig-summary-bell-curve width="40%" fig-align="left" alt="Bell shaped curve centred over 100"}

How close they are to the average depends on the number of sales made (the sample size) and the variance (or spread) of sales values.
The further away from the average week our observed value is, the more likely it is that the customers from that week were unusual---again depending on the size and spread of our sales.
And this is what our margin of error is trying to help us understand.

Whew!
That's quite a lot to chew on.
But, Boss wants more.

Let's take a break and freshen up before things become *(even more)* theoretical.

## Part 2 --- But does it matter?

### Introduction

In this second part we will take a look at another way to tell if the observed difference is meaningful.
We shall do this to begin to understand where our previous understanding comes from and compare it to the confidence intervals we are already familiar with.

### Boss is back

The Boss is fair impressed with their new understanding of these concepts, but wants some extra assurance that this is not just some fluke week.
To this effect, Boss has heard tell of a **magic number** that "*just tells you*" if the new customers are different from the old.

::: columns
::: column
![](../images/vb_p-998.png){width="210" fig-align="center" alt="Value box showing a p-value of 0.998. The title bar is blue indicating the results is non-significant."}

![](../images/vb_pval-sig.png){width="213" fig-align="center" alt="Value box showing a p-value of 0.013. The title bar is green indicating the results is different from the expected result to a meaningful degree."}
:::

::: column
Under the plots is a row of four boxes containing a value.
This magical number is displayed in the left-most box.
This number is closely related to the confidence interval, and is similarly burdened with some heinously (but necessarily) obtuse language.
This is a metric that asks:

> if the customer base **had not** changed, how unusual would the data obtained be?
:::
:::

The more weird the data is---compared to our usual data---the more different the new average will be from the old average, and the **smaller** this magic number is.
It works on pass/fail basis based on a hard threshold.
Without delving any more deeply into the stats, we can say that this threshold is determined in the same way as the CI.
Each margin of error is associated with a probability, which

-   If we choose an MoE of 90%, that corresponds to a probability of 0.9, which gives a threshold of 0.1
-   A MoE of 95% corresponds to a probability of 0.95, and give a threshold of 0.05
-   A MoE of 99% corresponds to a probability of 0.99, and give a threshold of 0.01
-   In other words subtract the probability from 1 to find the threshold (usually called alpha or $\boldsymbol{\alpha}$).

::: {#box-diff-pop .callout-tip icon="'false"}
### **`r i('Discuss')`** {#compare-pops}

If we have successfully shifted our customer base we might see a new distribution of weekly sales (e.g. @fig-dist-comparison).
Whether or not our magic number told us this was a meaningful change would depend on our sample size and spread, as we have already seen.

Discuss how this relates to our previous discussion on the [best CI level](#box-best-ci-disc)

![Comparison of old (black) and new (red) weekly sales distributions indicating that our new customers do indeed come from a different base.](../images/d2-s1_plot_part-2_bell-comparison.png){#fig-dist-comparison width="40%" fig-align="left" alt="Two bell shaped curves, one centred over 100, another, narrower, one centred over 140."}
:::

::: {.callout-tip icon="false"}
### **`r i('Task')`**

Using what we have learned so far, see if you can create a situation where this "P value" tells us that our new customers are in fact different from out old ones.
Depending on the random component of your app, this may be difficult.
You will know when you have managed because the bar under the box where "P value" is written will change colour.

If you manage this, look at the CI box to the right, look at the error bars on the plot above, and take note of the location of the red points on the two graphs on the right (the CDF and PDF).

![](../images/plot_distrs-sig-neg.png){width="40%" alt="Two plots stacked verically. Top: an \"s\" shaped curve with the y axis ranging between 0 and 1. Bottom: a bell shaped curve with the y axis ranging from 0 to 0.4. The x axes range from -4 to 4. There is a point on each curve at x = -3, and is very close to 0 on the y axis. These points are shown to be further left than the significance threshold, shown at -2"}
:::

The p-value is a useful metric, but the importance of it is easily and often overstated.
We are going to try and understand it in the context of these other two graphs: the **cumulative distribution function** (CDF) and the **probability density function** (PDF).

### The CDF {#the-cdf}

Unlike many things in statistics (p value? confidence interval?) the CDF is perhaps less complicated than might be expected.
To explore it we shall make another table from our data.

::: {.callout-tip icon="false"}
#### **`r i('Task')`**

Make a table like the one below.
In the example there are enough rows to record 6 observations -- you may want a few more.

![](../images/pval-table.png){width="75%" align="center"}

```{r}
#| label: tbl-distr-table
#| tbl-cap: Table for recording the probabilities of observed values
# distr_vals
```

The table is split into two sections.
The top part is labelled **Below** and should be used for recording sales values that come in under the expected value of the old customer base.

The second section labelled **Above** is for recording values above the old expectation.

Fill the columns as labelled.
The **Observed** value can be read off the left hand plot, and the remaining values can be taken from the appropriate boxes under the plots.

Hit ![](../images/btn_new-sample.png){width="107" alt="\"New sample\""} enough times to fill your table.

#### Questions {.qqq}

-   Focusing on the **Below** section of the table, can you identify a pattern between the p-value and the CDF?
-   How does the pattern in the **Above** section compare?
-   Can you see any pattern between the **observed** value and either the p-value or the CDF?
:::

Hopefully, there is some hint of a relationship between the p-value and the CDF curve.
But what does it mean?
The $y$ axis ranges from 0 to 1.
These values are **probabilities** (it is impossible to have a probability outwith those bonds).
The value on the $y$ axis corresponding to the point is straight forward.
But what about the $x$ axis?
(If you cant find it, it is shared with the PDF, and is printed at the bottom of the bottom graph.) What does it represent?
The $x$ axis has been converted to a standardized scale.
A bit like when e.g. country statistics are standardized to be understood in a per capita context.
In this case the $x$ axis represents the distance of the new, observed, value from the old, expected, value.
Thus $x = 0$ represents no difference between them.

<!-- ::: {#fig-p-equals-1 layout="[[3,[[1],[1],[1]]]]"} -->

<!-- ![](../images/plot--p-1.png) -->

<!-- ![](../images/vb--p-998.png) -->

<!-- ![](../images/vb--cdf_5.png) -->

<!-- ![](../images/vb--pdf_397.png) -->

<!-- ::: -->

<!-- ::: columns -->

<!-- ::: column -->

<!-- ![](../images/plot--p-1.png) -->

<!-- ::: -->

<!-- ::: column -->

<!-- ![](../images/vb--p-998.png) -->

<!-- ![](../images/vb--cdf_5.png) -->

<!-- ![](../images/vb--pdf_397.png) -->

<!-- ::: -->

<!-- ::: -->

Notice that when $x = 0$, $y = 0.5$.
This tells us that of all the values it is possible for the observation, 0.5, or 50% of them are less than the expected value.
So what about wherever your point is?
Follow the point down to the $x$ axis, and this is how far your observed value deviates from the expected value, in these, aforementioned, standardized units.
The corresponding value on the $y$ axis is the probability of obtaining the current value, or any value even further to the left---a value that is **more extreme** in how far it deviates from the expected value.

::: {.callout-tip icon="false"}
#### **`r i('Discuss')`**

Hit the ![](../images/btn_new-sample.png){width="108" alt="\"New sample\""} button a few times, use what we have already learned today to try and get a p-value below the threshold.

If your observed value happens to be below the expected value, things might start to make sense.

A low p-value will put the point low on the curve and far enough left as to be out side the yellow vertical line.

However, spare a thought for those whose observed value is significantly above the expected value.

In those cases, the p-value will be very low, but the point will be high on the curve and far to the right.

-   How should this apparent contradiction be interpreted?
-   Think about the pattern(s) observed in your previous table. How does that fit with this "contradiction"?
-   If the graph should be interpreted as the probability of values further to the left than the point, can you see a way to figure out the probability of an observed value $x$ where $-1 < x \leq 0$
:::

### The PDF {#the-pdf}

The second curve on the right is slightly more difficult to grapple with just because the values on the $y$ axis are less intuitive; they are no longer probabilities.
At least the $x$ axis hasn't changed.
It is still in those same standardized units of how far the observation deviates from the expected average (you might call it a scale of **standard deviations**).

However, as for the $y$ axis units, those are **densities** (?!?) but thankfully we do not need to go any deeper into that for our purposes.
The plot itself has to be understood in the context of the **area** under the entire curve.
It can still be interpreted in a similar way to the CDF.
However, instead of adding up all the values further to the left we consider the area under two points.
If those two points are so close together, that we can't really distinguish them, then that is close enough for us to consider the area under a single observed point.

It turns out that if our two points happen to be at $\pm1$ standard deviation, then the area of the curve within that range is about 68% of the total area under the entire curve.
Even more interesting is that 95% of the total area under the curve is between $\pm2$ standard deviations.

::: {.callout-tip icon="false"}
#### **`r i('Discussion')`**

While the CI makes perfectly reasonable sense in the cumulative function, above, the density function allows to come to a slightly different understanding.

A common benchmark for declaring a difference between two groups (such as our old, and our new customer bases) is that their respective averages will be at least 2 standard deviations away from each other.

Play around with the different options for our margin of error.
- How do the different MoE's compare to that convention.
- Note that it is only a convention, and that we are free to use whatever threshold we like to declare that we are indeed tapping into a different customer base.
- Think again about the previously mentioned "contradiction".
If the area under ±2 standard deviations account for 95% of the total area, how much must be beyond two standard deviations on **each side**?
:::

One final point to bring us back around, full circle.
Pay close attention to the shape of the PDF curve, and change the sample size between, say 5, 10, 20 and 50.
We should notice two things.
Firstly, the shape of the curve changes, and secondly the position of the yellow CI lines move.

::: {.callout-tip icon="false"}
#### **`r i('Questions')`**

-   What are the implications of these changes?
-   What do they imply about the relationship between the sample size, our CI, and our ability to declare a true difference between our two customer bases?
:::

## The Boss want some answers

Boss rocks up one final time.
They want some **definitive** answers to the following questions:

1.  Did the marketing campaign succeeded in bringing in a different customer base?
2.  If so, would we class it as a success?
3.  What is your best estimate for the new average sale for the new customer base?
    -   You might want to consider some nuance here.
    -   Do you really want to put your neck on the line by giving a single estimated value?
4.  The Boss wants to try a new marketing campaign.
    -   Hit the ![](../images/btn_new-campaign){width="122" height="40" alt="New campaign"} button at the top of the menu bar to target a different customer base, and start again!

## Conclusion / summary

### Summary of main learning points

We have covered *a lot* of concepts here.
Let's get together and summarize them all with the instructors.

-   Explored effects of [sample size](#sample-size)
-   Explored effects of [variance](#precicion)
-   Developed an understanding of [confidence intervals](#Margin-of-error)
-   Developed an awareness that different levels of confidence bring certain trade-offs.
    -   i.e. how much are you willing to accept incorrect estimates.
-   Explored how to tell if a sample belongs to the same population or a different one ([difference in populations](#box-diff-pop)).
-   begun to develop an awareness of distribution functions
-   begun to understand how the distribution function relates to a business question ([CDF](#the-cdf) and [PDF](#the-pdf))
-   and how they can be used to inform business decisions.

## Conclusion

By experimenting with the way sample size and variance interact we have seen how engaging with uncertainty helps us learn more about the processes of importance to us and our work.

Over time, we can develop a deeper appreciation of these concepts.
Eventually, we will learn to incorporate our new understanding and apply it to our decision making processes, and become better at our own jobs.

As we learn to become more comfortable embracing uncertainty, we can create different strategies for those times when variance bites hard.

If we learn to embrace it better than our competitors, we can create options more quickly than they can, and just be better than them!
